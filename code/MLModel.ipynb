{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO7A+LLvs3bue6eEFj9rP/y"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"bagYa3lSPE6w","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686480026121,"user_tz":-420,"elapsed":8124,"user":{"displayName":"waiwit charoenwilatpong","userId":"11347135891525513878"}},"outputId":"65053c68-09cf-496e-b5f5-724b6c092f9e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading...\n","From: https://drive.google.com/uc?id=1hqHARw2rrB_Y9gaz_wLuaSSd6DhoWqC0\n","To: /content/mockup_ecomm_cust_profile-202303.csv\n","100% 577k/577k [00:00<00:00, 83.4MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1WAhgUE0ctA6C477rbrfiCs3nH1Xh3uHT\n","To: /content/mockup_ecomm_cust_profile.csv\n","100% 369k/369k [00:00<00:00, 130MB/s]\n"]}],"source":["# !gdown 1hqHARw2rrB_Y9gaz_wLuaSSd6DhoWqC0\n","# !gdown 1WAhgUE0ctA6C477rbrfiCs3nH1Xh3uHT"]},{"cell_type":"markdown","source":["#Dependence library"],"metadata":{"id":"ubYWzoB7hV1t"}},{"cell_type":"code","source":["import pandas as pd;\n","import numpy as np;\n","import json;\n","import re;\n","import joblib;\n","\n","from sklearn.cluster import KMeans;\n","from sklearn.metrics import silhouette_score;\n","from sklearn.decomposition import PCA;\n","from sklearn.ensemble import RandomForestClassifier;\n","from sklearn.preprocessing import MinMaxScaler;"],"metadata":{"id":"MsYfp_GTejsi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Customer Classification"],"metadata":{"id":"aHEg2XZQhVgV"}},{"cell_type":"code","source":["class CustomerClassification:\n","    RAND_SEED = 3141592857;\n","\n","    customer_profile:pd.DataFrame = None;\n","    trainset: pd.DataFrame = None;\n","\n","    model:KMeans = None;\n","    model_k:int = 4\n","\n","\n","    def set_profile(self, df:pd.DataFrame):\n","        self.customer_profile = df;\n","        return self;\n","    \n","    def set_model_k(self, k:int):\n","        self.model_k = k;\n","        return self;\n","\n","    def get_profile(self) -> pd.DataFrame: return self.customer_profile.loc[:];\n","    def get_trainset(self) -> pd.DataFrame: return self.trainset.loc[:];\n","\n","\n","    def prepare_trainset(self):\n","        def flag_outlier(x:float, q1:float, q3:float, iqr:float)->int:\n","            if x < q1 - 1.5*iqr: return -1; #Extreme low,\n","            if x > q3 + 1.5*iqr: return 1; #Extreme hi,\n","            return 0;\n","        \n","        def get_dummies_recent_flag(df:pd.DataFrame)->pd.DataFrame:\n","            if df.empty: return df;\n","            df.set_index('id', inplace=True);\n","            dummies:pd.DataFrame = df['recent_flag'].apply(lambda x: 'recent_flag__'+x).str.get_dummies();\n","            if 'recent_flag__unknown' in dummies.columns: dummies.drop(columns=['recent_flag__unknown'], inplace=True);\n","            df = df.join(dummies).drop(columns=['recent_flag']);\n","            return df.reset_index(names='id');\n","        \n","        def explode_category_p(df:pd.DataFrame)->pd.DataFrame:\n","            if df.empty: return df;\n","            df.set_index('id', inplace=True);\n","            cat:pd.DataFrame = ( df[ df['category_p'].notna() ]['category_p']\n","                    .apply(lambda x:json.loads(x)['category_prob'])\n","                    .transform(pd.Series) );\n","            cat = cat.reindex( sorted(cat.columns), axis='columns' );\n","            df = df.join(cat).drop(columns=['category_p']).fillna(0);\n","            return df.reset_index(names=['id']);\n","        \n","        def normalize(df:pd.DataFrame)->pd.DataFrame:\n","            if df.empty: return df;\n","            df.set_index('id', inplace=True);\n","            df[df.columns] = MinMaxScaler().fit_transform( df[df.columns] );\n","            return df.reset_index(names=['id']);\n","\n","        prep:pd.DataFrame = self.customer_profile.drop(columns=['reg_date', 'latest_purchase']);\n","        prep.drop(prep[prep['recent_flag']=='unknown'].index, inplace=True);\n","\n","        pkt_sz__q1 = prep['pkt_sz'].quantile(.25);\n","        pkt_sz__q3 = prep['pkt_sz'].quantile(.75);\n","        pkt_sz__iqr = pkt_sz__q3 - pkt_sz__q1;\n","        prep['pkt_sz__outlier_f'] = prep['pkt_sz'].apply( lambda x: flag_outlier(x, pkt_sz__q1, pkt_sz__q3, pkt_sz__iqr) );\n","\n","        gap__q1 = prep['gap'].quantile(.25);\n","        gap__q3 = prep['gap'].quantile(.75);\n","        gap__iqr = gap__q3 - gap__q1;\n","        prep['gap__outlier_f'] = prep['gap'].apply( lambda x: flag_outlier(x, gap__q1, gap__q3, gap__iqr) );\n","\n","        # Remove outlier\n","        prep = prep[ (prep['pkt_sz__outlier_f']==0) & (prep['gap__outlier_f']==0) ].drop(columns=['pkt_sz__outlier_f', 'gap__outlier_f'] ).loc[:];\n","\n","        # Dummies recency\n","        prep = get_dummies_recent_flag(df=prep.loc[:]);\n","\n","        # Explode category probability\n","        prep = explode_category_p(df=prep.loc[:]);\n","        \n","        # Normalize data: min-max\n","        prep = normalize(df=prep.loc[:]);\n","\n","        self.trainset = prep.loc[:];\n","        return self;\n","    \n","\n","    def explore_models(self, normalized:pd.DataFrame, rng:range=range(2,8), rand_seed:int=None):\n","        '''Generate K-Means models with selection criteria'''\n","        def gen_models(k:int, df:pd.DataFrame, seed:int):\n","            np.random.seed(seed);\n","            return KMeans(n_init = 'auto', n_clusters=k).fit(df);\n","        \n","        ln = len(rng);\n","        k_metrix:pd.DataFrame = pd.DataFrame({\n","            'k': rng,\n","            'model': None,\n","            'wcss': None,\n","            'silhouette_avg': None,\n","        });\n","\n","        if rand_seed is None: rand_seed=self.RAND_SEED;\n","        normalized.set_index('id', inplace=True);\n","        k_metrix['model'] = k_metrix['k'].apply( lambda x: gen_models(x, normalized, rand_seed) );\n","        k_metrix['wcss'] = k_metrix['model'].apply(lambda x: x.inertia_);\n","        k_metrix['silhouette_avg'] = k_metrix['model'].apply(lambda x: silhouette_score(normalized, x.fit_predict(normalized)));\n","        normalized.reset_index(names=['id'], inplace=True);\n","        return k_metrix.loc[:];\n","    \n","\n","    def explore_model__wcc_silhouette(self, k_metrix:pd.DataFrame):\n","        from matplotlib import pyplot as plt;\n","\n","        fig, ax1 = plt.subplots();\n","        color = 'tab:blue';\n","        ax1.set_xlabel('k');\n","        ax1.set_ylabel('wcss', color=color);\n","        ax1.plot(k_metrix['k'], k_metrix['wcss'], color=color);\n","        ax1.tick_params(axis='y', labelcolor=color);\n","\n","        ax2 = ax1.twinx();\n","\n","        color = 'tab:brown';\n","        ax2.set_ylabel('silhouette', color=color);\n","        ax2.plot(k_metrix['k'], k_metrix['silhouette_avg'], color=color);\n","        ax2.tick_params(axis='y', labelcolor=color);\n","\n","        fig.tight_layout();\n","        plt.title('k-means: criteria');\n","        plt.show();\n","        pass;\n","\n","\n","    def explore_model__cluster_visual(self, df:pd.DataFrame, k_metrix:pd.DataFrame, rand_seed:int=None):\n","        from matplotlib import pyplot as plt;\n","\n","        if rand_seed is None: rand_seed=self.RAND_SEED;\n","\n","        df.set_index('id', inplace=True);\n","        pca:PCA = PCA(n_components=2);\n","        pca_xy:pd.DataFrame = pd.DataFrame(pca.fit(df).transform(df), columns=['x','y']);\n","\n","        k_metrix['idx'] = k_metrix.index;\n","        k_metrix['ax_col'] = k_metrix['idx']%4;\n","        k_metrix['ax_row'] = k_metrix['idx']/4;\n","        k_metrix['ax_row'] = k_metrix['ax_row'].apply(np.floor);\n","        k_metrix['ax_row'] = k_metrix['ax_row'].astype(int);\n","        k_metrix['ax_title'] = k_metrix['k'].apply(lambda x: 'Cluster @ k='+str(x));\n","\n","        height = (k_metrix['ax_row'].max() + 1)*2.4\n","        fig, axs = plt.subplots(k_metrix['ax_row'].max()+1, 4, figsize=(12.8, height));\n","        fig.tight_layout()\n","\n","        k_metrix.set_index('k', inplace=True);\n","        for i in k_metrix.index:\n","            np.random.seed(rand_seed);\n","            pca_xy['cluster'] = k_metrix.at[i,'model'].fit_predict(df);\n","            axs[ k_metrix.at[i,'ax_row'], k_metrix.at[i,'ax_col'] ].scatter(pca_xy['x'], pca_xy['y'], c=pca_xy['cluster'], alpha=0.5);\n","            axs[ k_metrix.at[i,'ax_row'], k_metrix.at[i,'ax_col'] ].set_title(k_metrix.at[i,'ax_title']);\n","            pass;\n","        k_metrix.reset_index(names=['k'],inplace=True);\n","        df.reset_index(names=['id'], inplace=True);\n","        pass;\n","\n","\n","    def explore_model__important_feature(self, clustered:pd.DataFrame, rand_seed:int=None) -> pd.DataFrame:\n","        if rand_seed is None: rand_seed=self.RAND_SEED;\n","        np.random.seed(rand_seed);\n","\n","        rf:RandomForestClassifier = RandomForestClassifier();\n","        clustered.set_index('id', inplace=True);\n","        rf.fit(clustered.drop(columns=['cluster']), clustered['cluster']);\n","        clustered.reset_index(names=['id'], inplace=True);\n","        feature_importance = rf.feature_importances_;\n","        col_order = feature_importance.argsort();\n","        return pd.DataFrame({\n","            'feature': clustered.drop(columns=['id']).columns[col_order],\n","            'importance': feature_importance[col_order]\n","        }).sort_values('importance', ascending=False, ignore_index=True);\n","\n","    \n","    def train_model(self, rand_seed:int=None):\n","        if rand_seed is None: rand_seed = self.RAND_SEED;\n","        df:pd.DataFrame = self.get_trainset();\n","\n","        np.random.seed(rand_seed);\n","        self.model = KMeans(n_init = 'auto', n_clusters=self.model_k).fit(df.drop(columns='id'));\n","        return self;\n","    \n","    def export_model(self, filename:str):\n","        joblib.dump(self.model, filename);\n","        return self;\n","\n","    def import_model(self, filename:str):\n","        self.model = joblib.load(filename);\n","        return self;\n","\n","    def execute_model(self, dataset:pd.DataFrame):\n","\n","        def get_dummies_recent_flag(df:pd.DataFrame)->pd.DataFrame:\n","            if df.empty: return df;\n","            df.set_index('id', inplace=True);\n","            dummies:pd.DataFrame = df['recent_flag'].apply(lambda x: 'recent_flag__'+x).str.get_dummies();\n","            if 'recent_flag__unknown' in dummies.columns: dummies.drop(columns=['recent_flag__unknown'], inplace=True);\n","            df = df.join(dummies).drop(columns=['recent_flag']);\n","            return df.reset_index(names='id');\n","        \n","        def explode_category_p(df:pd.DataFrame)->pd.DataFrame:\n","            if df.empty: return df;\n","            df.set_index('id', inplace=True);\n","            cat:pd.DataFrame = ( df[ df['category_p'].notna() ]['category_p']\n","                    .apply(lambda x:json.loads(x)['category_prob'])\n","                    .transform(pd.Series) );\n","            cat = cat.reindex( sorted(cat.columns), axis='columns' );\n","            df = df.join(cat).drop(columns=['category_p']).fillna(0);\n","            return df.reset_index(names=['id']);\n","        \n","        def normalize(df:pd.DataFrame)->pd.DataFrame:\n","            if df.empty: return df;\n","            df.set_index('id', inplace=True);\n","            df[df.columns] = MinMaxScaler().fit_transform( df[df.columns] );\n","            return df.reset_index(names=['id']);\n","\n","        def feature_desc(clustered:pd.DataFrame, k:int)->pd.DataFrame:\n","            col='cluster';\n","            # Filter features\n","            desc:pd.DataFrame = ( clustered[ clustered[col]==k ].drop(columns=[col]).describe().transpose()\n","                    .sort_values(['50%', 'mean'], ascending=False) );\n","            desc = desc[ desc['50%']>0.05 ].head(6).loc[:];\n","\n","            # Order columns\n","            cols:pd.Series = pd.Series(['recent_flag__active', 'recent_flag__inactive', 'recent_flag__churn', 'patron_length',\n","                    'pkt_sz', 'dc_amnt', 'gap', 'p_purchase', 'p_purchase_dc'], name='feature');\n","            imp_feature:list = desc.index.to_list();\n","            imp_feature = cols[cols.isin(imp_feature)].to_list() + re.findall(r'\\bCAT[0-9]*\\b',' '.join(imp_feature));\n","            desc = desc.reindex(imp_feature);\n","            return desc.loc[:]\n","\n","        def label(df:pd.DataFrame)->str:\n","            def fn(x:pd.DataFrame)->str:\n","                if x['label'] in ['STATUS-ACTIVE', 'STATUS-INACTIVE', 'STATUS-CHURN']:\n","                    return '{}:{}%'.format(x['label'], round(x['mean'], 3)*100);\n","\n","                c1:str = 'L' if x['25%'] < .33 else 'H' if x['25%'] > .67 else 'M';\n","                c2:str = 'L' if x['75%'] < .33 else 'H' if x['75%'] > .67 else 'M';\n","                return '{}:{}{}'.format(x['label'], c1, c2);\n","\n","            col_name_map:dict = {\n","                'recent_flag__active': 'STATUS-ACTIVE',\n","                'recent_flag__inactive': 'STATUS-INACTIVE',\n","                'recent_flag__churn': 'STATUS-CHURN',\n","                'patron_length': 'PATRON-LEN',\n","                'pkt_sz': 'POCKET-SIZE',\n","                'dc_amnt': 'DISCOUNT',\n","                'gap':'GAP-BTW-PURCHASE',\n","                'p_purchase': 'PURCHASE-PROB',\n","                'p_purchase_dc': 'DISCOUNT-PROB'\n","            }\n","\n","            df = df.reset_index(names=['col']);\n","            df['label'] = df['col'].map(col_name_map);\n","            df['label'] = df['label'].combine_first(df['col']);\n","            df['label'] = df[['label', 'mean', '25%', '75%']].apply(fn, axis='columns');\n","            return ' | '.join(df['label']);\n","\n","        dataset = dataset.loc[:];\n","        dataset = explode_category_p(dataset);\n","        df:pd.DataFrame = dataset.drop(columns=['reg_date', 'latest_purchase']);\n","        df = ( df.pipe(get_dummies_recent_flag)\n","                .pipe(normalize) );\n","        \n","        np.random.seed(self.RAND_SEED);\n","        df['cluster'] = self.model.fit_predict(df.drop(columns=['id']));\n","\n","        segment:pd.DataFrame = pd.DataFrame(\n","            index=df['cluster'].sort_values().drop_duplicates(),\n","            data={\n","                'c':None,\n","                'feature': None,\n","                'label': None\n","            });\n","        segment['c'] = segment.index;\n","        segment['feature'] = segment['c'].apply(lambda x: feature_desc(df, x));\n","        segment['label'] = segment['feature'].apply(lambda x: label(x));\n","        segment_label:dict = segment[['label']].to_dict()['label'];\n","        \n","        df['segment'] = df['cluster'].map(segment_label);\n","        customer_cluster:dict = df.set_index('id')['cluster'].to_dict();\n","        dataset['segment_id'] = dataset['id'].map(customer_cluster);\n","        customer_segment:dict = df.set_index('id')['segment'].to_dict();\n","        dataset['segment'] = dataset['id'].map(customer_segment);\n","        return dataset.loc[:];"],"metadata":{"id":"DApxaczCeqGR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Example"],"metadata":{"id":"ctuwh1fu4VQZ"}},{"cell_type":"markdown","source":["##Sample of model development"],"metadata":{"id":"XlruV7aQksTa"}},{"cell_type":"code","source":["# # Prepare train-dataset\n","# src:pd.DataFrame = pd.read_csv('mockup_ecomm_cust_profile-202303.csv');\n","# cc:CustomerClassification = ( CustomerClassification().set_profile(src)\n","#         .prepare_trainset() );\n","# prep = cc.get_trainset();\n","\n","# # Generate model and k - criteria\n","# k_metrix = cc.explore_models( prep, range(2,20) );\n","# display( cc.explore_model__wcc_silhouette(k_metrix),\n","#     prep.boxplot(['patron_length', 'pkt_sz', 'dc_amnt', 'gap', 'p_purchase', 'p_purchase_dc']),\n","#     cc.explore_model__cluster_visual(prep,k_metrix) )\n","\n","# # Define cluster characteristic - customer segment\n","# prep['cluster'] = k_metrix.set_index('k').at[4, 'model'].fit_predict(prep.drop(columns=['id']))\n","# display( cc.explore_model__important_feature(prep) )\n","# for x in prep['cluster'].drop_duplicates().sort_values():\n","#     segment_desc = ( prep[prep['cluster']==x].drop(columns=['cluster']).describe().transpose()\n","#             .sort_values(['50%', 'mean'], ascending=False) );\n","#     display('cluster: '+str(x), segment_desc)"],"metadata":{"id":"HLKEB3XVDwo6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Sample of model testing"],"metadata":{"id":"ctMI8hBhkaYG"}},{"cell_type":"code","source":["# src:pd.DataFrame = pd.read_csv('mockup_ecomm_cust_profile-202303.csv');\n","# tgt:pd.DataFrame = pd.read_csv('mockup_ecomm_cust_profile-202303.csv');\n","# cc:CustomerClassification = ( CustomerClassification().set_profile(src)\n","#         .prepare_trainset()\n","#         .set_model_k(4).train_model() );\n","# df = cc.execute_model(tgt)"],"metadata":{"id":"hrJelgMZ2DsL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Sample of export, deploy, and execute"],"metadata":{"id":"h0tiuwGz8R1N"}},{"cell_type":"code","source":["# src:pd.DataFrame = pd.read_csv('mockup_ecomm_cust_profile-202303.csv');\n","# tgt:pd.DataFrame = pd.read_csv('mockup_ecomm_cust_profile-202303.csv');\n","# ( CustomerClassification().set_profile(src).prepare_trainset()\n","#         .set_model_k(4).train_model()\n","#         .export_model('model.joblib') );\n","\n","# df:pd.DataFrame = ( CustomerClassification().import_model('model.joblib')\n","#         .execute_model(tgt) )\n","# df.segment.drop_duplicates()"],"metadata":{"id":"7ND6hDLv8aym"},"execution_count":null,"outputs":[]}]}